<!DOCTYPE html>
<html lang="en"><head>
<script src="slides_files/libs/clipboard/clipboard.min.js"></script>
<script src="slides_files/libs/quarto-html/tabby.min.js"></script>
<script src="slides_files/libs/quarto-html/popper.min.js"></script>
<script src="slides_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="slides_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="slides_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="slides_files/libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.40">

  <title>slides</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="slides_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="slides_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="slides_files/libs/revealjs/dist/theme/quarto-423d414d36ce366a9d05f36d06304417.css">
  <link rel="stylesheet" href="style.css">
  <link href="slides_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="slides_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="slides_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="slides_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">


<section class="slide level2">

<div class="slide" style="text-align:center;">
<p><img data-src="images/logo_sfds.png" style="width:6cm" alt="logo_sfds <"> <img data-src="images/logo_stat_sport.png" style="width:2cm" alt="logo_stat_sport >"></p>
<hr>

<section id="analyse-de-données-fonctionnelles-pour-des-problématiques-sportives-spécificités-approches-classiques-et-probabilistes" class="title-slide slide level1 center">
<h1>Analyse de données fonctionnelles pour des problématiques sportives : spécificités, approches classiques et probabilistes</h1>
<hr>

</section>
<h4 id="arthur-leroy---gabi-mia-paris-saclay-inrae"><strong>Arthur Leroy</strong> - GABI &amp; MIA Paris Saclay, INRAE</h4>
<p style="text-align: center;">
<img src="images/QR_code_website.png" width="150px">
</p>
<h4 id="ateliers-statistique-sport---15102025">Ateliers Statistique &amp; Sport - 15/10/2025</h4>

</div>
</section>
<section id="can-you-spot-the-differences" class="slide level2">
<h2>Can you spot the differences?</h2>
<center>
<img data-src="images/data_multivariate.png" style="width:85.0%">
</center>
</section>
<section id="can-you-spot-the-differences-1" class="slide level2">
<h2>Can you spot the differences?</h2>
<img data-src="images/data_longitudinal.png" style="width:100.0%">

</section>
<section id="functional-data-is-all-about-smoothness-and-tidiness" class="slide level2">
<h2>Functional data is all about smoothness and tidiness</h2>
<center>
<img data-src="images/data_longitudinal_longer.png" height="550"> <img data-src="images/illu_longitudinal.png" style="width:60.0%">
</center>
</section>
<section id="functional-data-is-all-about-smoothness-and-tidiness-1" class="slide level2">
<h2>Functional data is all about smoothness and tidiness</h2>
<center>
<img data-src="images/data_longitudinal_longer.png" height="550"> <img data-src="images/illu_longitudinal_curve.png" style="width:60.0%">
</center>
</section>
<section id="functional-data-in-sports-time-space-and-continua" class="slide level2">
<h2>Functional data in sports ? Time, space and continua</h2>
Exemple de données GPS en rugby :
<center>
<img data-src="images/illu_gps_rugby.png" style="width:49.0%"> <img data-src="images/illu_gps_rugby_2_outputs.png" style="width:49.0%">
</center>
</section>
<section id="functional-data-appears-more-often-that-we-would-imagine" class="slide level2">
<h2>Functional data appears more often that we would imagine</h2>
<center>
<img data-src="images/barefoot_running.jpg" style="width:20.0%">   <img data-src="images/curve_running_barefoot.jpg" style="width:64.0%">
</center>
<p><strong>Zech et al.</strong> - <em>Effects of barefoot and footwear conditions on learning of a dynamic balance task: a randomized controlled study</em> - EJAP - 2018</p>
<p><strong>Hollander et al.</strong> - <em>Adaptation of Running Biomechanics to Repeated Barefoot Running: A Randomized Controlled Study</em> - TAJSM - 2019</p>
</section>
<section id="like-what-would-have-happened-to-results-without-covid" class="slide level2">
<h2>Like: what would have happened to results without Covid?</h2>
<p><br></p>
<center>
<img data-src="images/Fina_points_avec_covid_data.png" style="width:49.0%"> <img data-src="images/Fina_points_sans_covid_data.png" style="width:49.0%">
</center>
<p><strong>Veiga, S., Grenouillat, A., et al.</strong> - Ten-Year Evolution of World Swimming Trends for Different Performance Clusters: A Gaussian Model - IJSPP - 2024</p>
</section>
<section id="now-you-know.-functional-data-2020" class="slide level2">
<h2>Now you know. Functional data &gt; 2020</h2>
<center>
<img data-src="images/MagmaClust_Covid.png" style="width:100.0%">
</center>
</section>
<section id="lets-start-with-an-easy-problem-computing-cumulated-workload" class="slide level2">
<h2>Let’s start with an easy problem : computing cumulated workload</h2>
<center>
<img data-src="images/redi_plot.png" style="width:55.0%">
</center>
<p>For discrete time series, we can be interested in tracking the <strong>cumulated workload</strong> over time. This can be seen as a discrete integral of successive measurements.</p>
</section>
<section id="naive-approaches-can-be-useful-but-are-often-limited" class="slide level2">
<h2>Naive approaches can be useful but are often limited</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><span class="math inline">\(ACWR(t) = \dfrac{\bar{L}_{week}(t)}{\sum\limits_{i=0}^{3} \bar{L}_{week}(t - 7i)}\)</span></p>
<p><br> <span class="math inline">\(EWMA(t) = \lambda_N L(t) + (1- \lambda_N) \text{EWMA}(t-1)\)</span></p>
<p>with: <span class="math inline">\(\lambda_N = \dfrac{2}{N + 1}\)</span></p>
</div><div class="column" style="width:50%;">
<center>
<img data-src="images/EWMA_ACWR_missing.png" style="width:100.0%">
</center>
</div></div>
<p><br></p>
<p><strong>EWMA</strong> (<em>Exponential Weighted Moving Average</em>) and <strong>ACWR</strong> (<em>Acute Chronic Workload Ratio</em>) have been proposed to compute cumulated workload, but <span class="orange">mathematical problems</span> (sensible to missing data, biased, fixed decreasing behaviour) prevent their direct use in practice.</p>
</section>
<section id="understanding-properties-of-functions-to-develop-adapted-tools" class="slide level2">
<h2>Understanding properties of functions to develop adapted tools</h2>
<p><strong>REDI</strong> (<em>Robust Exponential Decreasing Index</em>) has been developed to overcome those issues by looking at this problem from a functional perspective. Computing (discrete) integrals from measurements allow us to track cumulated workload in a more <span class="orange">robust</span> and <span class="orange">flexible</span> way.</p>
<center>
<img data-src="images/REDI_formula.png" style="width:35.0%"><img data-src="images/REDI_missing.png" style="width:64.0%">
</center>
<p>Computed with the R package <strong>REDI</strong> or the web app <a href="https://arthurleroy.shinyapps.io/REDI/" class="uri">https://arthurleroy.shinyapps.io/REDI/</a>.</p>
</section>
<section id="practice-time" class="slide level2">
<h2>Practice time!</h2>
<p>We can switch to the notebook for the first time and start practicing.</p>
<center>
<img data-src="images/gif_REDI.gif" style="width:80.0%">
</center>
</section>
<section id="reconstructing-functions-from-series-of-points" class="slide level2">
<h2>Reconstructing functions from series of points</h2>
A function can be expressed as a linear combination of basis functions: <span class="math display">\[f(t)  = \sum\limits_{b=1}^{B}{\alpha_b \ \phi_b(t)}\]</span>
<center>
<img data-src="images/b-splines.jpg" style="width:45.0%">
</center>
<p>If we observed the function at <span class="math inline">\(N\)</span> instants, we can find coefficients <span class="math inline">\(\boldsymbol{\alpha}\)</span> through least squares:</p>
<p><span class="math display">\[LS(\boldsymbol{\alpha}) = \sum_{i=1}^{N}\left[f(t_i)-\sum_{b = 1}^{B} \alpha_{b} \phi_{b}\left(t_{i}\right)\right]^{2}\]</span></p>
</section>
<section id="functional-data-analysis-is-the-art-of-drawing-curves-from-points" class="slide level2">
<h2>Functional data analysis is the art of drawing curves from points</h2>
<p>Depending on the context, we may expect <strong>different properties</strong> for our function.</p>
<center>
<img data-src="images/FDA_book_Ramsay.jpg" style="width:20.0%"><img data-src="images/interpolation_smoothing.png" style="width:80.0%">
</center>
<p>Do we interpolate or smooth? Are the variations periodic (Fourier basis), multi-scale (wavelets) or polynomial (B-splines)? Answers are probably is <span class="orange">this book</span>.</p>
</section>
<section id="functional-principal-component-analysis-fpca" class="slide level2">
<h2>Functional Principal Component Analysis (FPCA)</h2>
<p>Like for multi-variate statistics, <span class="orange">FCPA</span> is central method when studying functions. From the <em>Karhunen-Loève</em> theorem we can express any centred stochastic process as an infinite linear combination of orthonormal eigenfunctions:</p>
<p><span class="math display">\[X(t)-\mathbb{E}[X(t)]=\sum_{q=1}^{\infty} \xi_{q} \varphi_{q}(t)\]</span></p>
<center>
<img data-src="images/elbow_FPCA.png" style="width:45.0%">
</center>
</section>
<section id="i-must-admit-its-a-bit-trickier-to-interpret" class="slide level2">
<h2>I must admit, it’s a bit trickier to interpret</h2>
<p>It’s still used to represent the trajectories <span class="orange">explaining the most variance</span> among our functions. Eigenfunctions are <strong>uncorrelated</strong>, and they provide the most <strong>parsimonious</strong> decomposition in terms of basis function.</p>
<center>
<img data-src="images/eigenfunctions_FPCA.png" style="width:60.0%">
</center>
</section>
<section id="people-are-doing-many-more-or-less-crazy-things-trust-me" class="slide level2">
<h2>People are doing many (more or less crazy) things, trust me</h2>
<p>An important strategy is <span class="orange">non-parametric FDA</span> that assumes no finite decomposition and instead defines specific metrics to measure a <em>distance</em> between functions directly.</p>
<center>
<img data-src="images/FDA_book_Ferraty.jpg" style="width:20.0%">               <img data-src="images/FDA_book_mixed_models.jpg" style="width:20.0%">
</center>
<p><span class="orange">Longitudinal mixed models</span> are also worth a mention as including time as an input variable (also called fixed effect) in a mixed/hierarchical model proved to be efficient in many applications.</p>
</section>
<section id="the-curse-of-dimensionality-dont-care-im-smooth" class="slide level2">
<h2>The curse of dimensionality? Don’t care, I’m smooth</h2>
<p>One nice and surprising property of FDA methods is that they generally don’t suffer from being <em>infinitely</em> high dimensional. Intuitively, what is the <em>true</em> dimension of this object?</p>
<center>
<img data-src="images/constant_function.png" style="width:80.0%">
</center>
</section>
<section id="what-if-i-observed-multiple-functions-lets-make-groups" class="slide level2">
<h2>What if I observed multiple functions? Let’s make groups</h2>
<p>One classical problem in FDA (and statistics in general) is <span class="orange">clustering</span>: allocating functions that seem similar into groups according to some property. There are different strategies:</p>
<center>
<img data-src="images/clustering_FDA_families.png" style="width:60.0%">
</center>
</section>
<section id="modelling-functions-gives-you-derivatives-to-study-dynamics" class="slide level2">
<h2>Modelling functions gives you derivatives to study dynamics</h2>
<p>Clustering performance curves of 100m freestyle swimmers into 5 groups using derivatives.</p>
<center>
<img data-src="images/multclust.png" style="width:100.0%">
</center>
</section>
<section id="time-for-some-practice" class="slide level2">
<h2>Time for some practice</h2>
<center>
<img data-src="images/illu_spline_TP.png" style="width:80.0%">
</center>
</section>
<section id="fda-is-limited-and-its-kinda-old-to-be-fair" class="slide level2">
<h2>FDA is limited (and it’s kinda old to be fair)</h2>
<center>
<img data-src="images/splines.png" style="width:80.0%">
</center>
<ul>
<li>While average trajectories of groups are reasonable, individual curves may easily diverge,</li>
<li>Really low predictive capacities,</li>
<li>No quantification of uncertainty.</li>
</ul>
</section>
<section id="probabilistic-modelling-and-predictions-how-do-we-really-learn" class="slide level2">
<h2>Probabilistic modelling and predictions: how do we really learn?</h2>
<center>
<img data-src="images/robot-reading-ai.jpg" style="width:60.0%">
</center>
</section>
<section id="moving-from-exploring-to-learning-functions" class="slide level2">
<h2>Moving from exploring to learning functions</h2>
<p><br></p>
<div class="block">
<p><span class="math display">\[y = \color{orange}{f}(x) + \epsilon\]</span></p>
</div>
<p>where:</p>
<div class="block">
<ul>
<li><span class="math inline">\(x\)</span> is the input variable (typically time, location, any continuum),</li>
<li><span class="math inline">\(y\)</span> is the output variable (typically performance measurements),</li>
<li><span class="math inline">\(\epsilon\)</span> is the noise, a random error term,</li>
<li><span class="math inline">\(\color{orange}{f}\)</span> is a <span class="orange">random function</span> encoding the <strong>relationship</strong> between input and output data.</li>
</ul>
</div>
<p><br></p>
<div class="fragment">
<p>All supervised learning problems require to retrieve the <em>most reliable</em> function <span class="math inline">\(\color{orange}{f}\)</span>, using observed data <span class="math inline">\(\{(x_1, y_1), \dots, (x_n, y_n) \}\)</span>, to perform <strong>predictions</strong> when observing new input data <span class="math inline">\(x_{n+1}\)</span>.</p>
</div>
</section>
<section id="learning-the-simplest-of-functions-linear-regression" class="slide level2">
<h2>Learning the simplest of functions : linear regression</h2>
<p>In the simplest (though really common in practice) case of linear regression, we assume that:</p>
<div class="block">
<p><span class="math display">\[\color{orange}{f}(x) = a x + b\]</span></p>
</div>
<p>Finding the best function <span class="math inline">\(\color{orange}{f}\)</span> reduces to compute optimal parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> from our dataset.</p>
<center>
<img data-src="images/linear_reg.gif" style="width:45.0%">
</center>
</section>
<section id="learning-is-about-updating-our-knowledge" class="slide level2">
<h2>Learning is about updating our knowledge</h2>
<p>This well-known probability formula has massive implications on learning strategies:</p>
<p><br></p>
<div class="block">
<p><span class="math display">\[\mathbb{P}(\color{red}{T} \mid \color{blue}{D}) = \dfrac{\mathbb{P}(\color{blue}{D} \mid \color{red}{T}) \times  \mathbb{P}(\color{red}{T})}{\mathbb{P}(\color{blue}{D})}\]</span></p>
</div>
<p>with:</p>
<div>
<ul>
<li class="fragment"><span class="math inline">\(\mathbb{P}(\color{red}{T})\)</span>, probability that some theory <span class="math inline">\(\color{red}{T}\)</span> is true, our <span class="orange">prior</span> belief.</li>
<li class="fragment"><span class="math inline">\(\mathbb{P}(\color{blue}{D} \mid \color{red}{T})\)</span>, probability to observe this data if theory <span class="math inline">\(\color{red}{T}\)</span> is true, the <span class="orange">likelihood</span>.</li>
<li class="fragment"><span class="math inline">\(\mathbb{P}(\color{blue}{D})\)</span>, probability to observe this data overall, often called the <span class="orange">evidence</span>.</li>
</ul>
</div>
<div class="fragment">
<p><span class="orange">Bayes’ theorem</span> indicates how to <strong>update</strong> our beliefs about <span class="math inline">\(\color{red}{T}\)</span> when accounting for new data <span class="math inline">\(\color{blue}{D}\)</span> :</p>
</div>
<div class="fragment">
<ul>
<li><span class="math inline">\(\mathbb{P}(\color{red}{T} \mid \color{blue}{D})\)</span>, probability that theory <span class="math inline">\(\color{red}{T}\)</span> is true considering data <span class="math inline">\(\color{blue}{D}\)</span>, our <span class="orange">posterior</span> belief.</li>
</ul>
</div>
</section>
<section id="a-visual-explanation-of-bayes-theorem" class="slide level2">
<h2>A visual explanation of Bayes’ theorem</h2>
<p>We generally use probability distributions to express our initial <span class="orange">uncertainty</span> about a quantity of interest (balance of a coin, average size of a human, …) and its posterior probable values.</p>
<center>
<img data-src="images/bayes_gif.gif" style="width:50.0%">
</center>
</section>
<section id="probabilistic-estimation-can-be-an-alternative-to-testing" class="slide level2">
<h2>Probabilistic estimation can be an alternative to testing</h2>
<center>
<img data-src="images/gif_example_posteriors.gif" style="width:90.0%">
</center>
</section>
<section id="lets-exercise-your-bayesian-mind" class="slide level2">
<h2>Let’s exercise your Bayesian mind</h2>
<p>Assume there exists some trouble or disease such that:</p>
<div>
<ul>
<li class="fragment"><span class="math inline">\(\mathbb{P}(\color{red}{T}) = 0.001,\)</span> 1 person out of 1000 contracted the <span class="red">trouble</span> on average,</li>
<li class="fragment"><span class="math inline">\(\mathbb{P}(\color{blue}{D} \mid \color{red}{T}) = 0.99,\)</span> a <span class="blue">detection</span> test is 99% reliable if you have the <span class="red">trouble</span>,</li>
<li class="fragment"><span class="math inline">\(\mathbb{P}(\color{blue}{\bar{D}} \mid \color{red}{\bar{T}}) = 0.99,\)</span> this same <span class="blue">detection</span> test is 99% reliable if you <strong>don’t</strong> have the <span class="red">trouble</span>,</li>
</ul>
</div>
<p><br></p>
<div class="fragment">
<p>From <span class="orange">Bayes’ theorem</span>, the probability to have contracted the <span class="red">trouble</span> when the <span class="blue">detection</span> test was positive is:</p>
</div>
<div class="fragment">
<div class="block">
<p><span class="math display">\[\mathbb{P}(\color{red}{T} \mid \color{blue}{D}) = \dfrac{\mathbb{P}(\color{blue}{D} \mid \color{red}{T}) \times  \mathbb{P}(\color{red}{T})}{\mathbb{P}(\color{blue}{D})} = \dfrac{0.99 \times 0.001}{0.99 \times 0.001 + (1-0.99) \times 0.999} \simeq 0.09\]</span></p>
</div>
<p><br></p>
</div>
<div class="fragment">
<p>Hence, we only have <span class="orange">9% chance</span> to actually be sick despite a positive result to the detection test.</p>
</div>
</section>
<section id="gaussian-process-a-prior-distribution-over-functions" class="slide level2">
<h2>Gaussian process: a prior distribution over functions</h2>
<div class="block">
<p><span class="math display">\[y = \color{orange}{f}(x) + \epsilon\]</span></p>
</div>
<div class="fragment">
<p><strong>No restrictions</strong> on <span class="math inline">\(\color{orange}{f}\)</span> but a <span class="orange">prior distribution</span> on a functional space: <span class="math inline">\(\color{orange}{f} \sim \mathcal{GP}(m(\cdot),C(\cdot,\cdot))\)</span></p>
</div>
<div class="fragment">
<p>We can think of a Gaussian process as the extension to infinity of multivariate Gaussians:<br>
      <span class="math inline">\(x \sim  \mathcal{N}(m , \sigma^2)\)</span> in <span class="math inline">\(\mathbb{R}\)</span>,       <span class="math inline">\(\begin{pmatrix}
                 x_1  \\
                 x_2 \\
               \end{pmatrix} \sim \mathcal{N} \left(
                \
              \begin{pmatrix}
                 m_1  \\
                 m_2  \\
               \end{pmatrix},
              \begin{bmatrix}
                 C_{1,1} &amp; C_{1,2} \\
                 C_{2,1} &amp; C_{2,2}
              \end{bmatrix} \right)\)</span> in <span class="math inline">\(\mathbb{R}^2\)</span></p>
</div>
<div class="fragment">
<center>
<img data-src="images/illu_gaussian_distrib.png" style="width:30.0%">      <img data-src="images/illu_2D_gaussian.jpg" style="width:40.0%"><br>
<em>Credits: Raghavendra Selvan</em>
</center>
</div>
</section>
<section id="a-gp-is-like-a-long-cake-and-each-slice-is-a-gaussian" class="slide level2">
<h2>A GP is like a long cake and each slice is a Gaussian</h2>
<center>
<img data-src="images/gp_slices.png" style="width:80.0%"><br>
<em>Credits: Carl Henrik Ek</em>
</center>
</section>
<section id="a-gp-is-like-an-infinitely-long-cake-and-each-slice-is-a-gaussian" class="slide level2">
<h2>A GP is like an <em>infinitely</em> long cake and each slice is a Gaussian</h2>
<center>
<img data-src="images/gp_slices2.png" style="width:80.0%"><br>
<em>Credits: Carl Henrik Ek</em>
</center>
</section>
<section id="covariance-functions-squared-exponential-kernel" class="slide level2">
<h2>Covariance functions: Squared Exponential kernel</h2>
<p>While <span class="math inline">\(m(\cdot)\)</span> is often assumed to be <span class="math inline">\(0\)</span>, the <span class="emphasized">covariance structure</span> is critical and defined through tailored <span class="emphasized">kernels</span>. For instance, the <em>Squared Exponential</em> (or RBF) kernel is expressed as: <span class="math display">\[C_{SE}(x, x^{\prime}) = s^2 \exp \Bigg(-\dfrac{(x - x^{\prime})^2}{2 \ell^2}\Bigg)\]</span></p>
<center>
<img data-src="images/illu_se_kernel.gif" height="380">
</center>
</section>
<section id="covariance-functions-periodic-kernel" class="slide level2">
<h2>Covariance functions: Periodic kernel</h2>
<p>To model phenomenon exhibiting repeting patterns, one can leverage the <em>Periodic</em> kernel: <span class="math display">\[C_{perio}(x, x^{\prime}) = s^2 \exp \Bigg(- \dfrac{ 2 \sin^2 \Big(\pi \frac{\mid x - x^{\prime}\mid}{p} \Big)}{\ell^2}\Bigg)\]</span></p>
<center>
<img data-src="images/illu_perio_kernel.gif" height="380">
</center>
</section>
<section id="covariance-functions-linear-kernel" class="slide level2">
<h2>Covariance functions: Linear kernel</h2>
<p>We can even consider linear regression as a particular GP problem, by using the <em>Linear</em> kernel: <span class="math display">\[C_{lin}(x, x^{\prime}) = s_a^2 +  s_b^2 (x - c)(x^{\prime} - c )\]</span></p>
<center>
<img data-src="images/illu_lin_kernel.gif" height="380">
</center>
<p>We can learn optimal values of <span class="emphasized">hyper-parameters</span> from data through <strong>maximum likelihood</strong>.</p>
</section>
<section id="gaussian-process-all-you-need-is-a-posterior" class="slide level2">
<h2>Gaussian process: all you need is a posterior</h2>
<p>The Gaussian property induces that unobserved points have <span class="orange">no influence</span> on inference:</p>
<div class="block">
<p><span class="math display">\[ \int \underbrace{p(f_{\color{grey}{obs}}, f_{\color{purple}{mis}})}_{\mathcal{GP}(m, C)} \ \mathrm{d}f_{\color{purple}{mis}} = \underbrace{p(f_{\color{grey}{obs}})}_{\mathcal{N}(m_{\color{grey}{obs}}, C_{\color{grey}{obs}})} \]</span></p>
</div>
<div class="fragment">
<p>This crucial trick allows us to learn function properties from finite sets of observations. More generally, Gaussian processes are closed under <span class="orange">conditioning and marginalisation</span>.</p>
</div>
<div class="fragment">
<div class="block">
<p><span class="math display">\[\begin{bmatrix}
                 f_{\color{grey}{o}} \\
                 f_{\color{purple}{m}} \\
               \end{bmatrix} \sim \mathcal{N} \left(
              \begin{bmatrix}
                 m_{\color{grey}{o}} \\
                 m_{\color{purple}{m}} \\
               \end{bmatrix},
              \begin{pmatrix}
                 C_{\color{grey}{o, o}} &amp; C_{\color{grey}{o}, \color{purple}{m}} \\
                 C_{\color{purple}{m}, \color{grey}{o}} &amp; C_{\color{purple}{m, m}}
              \end{pmatrix} \right)\]</span></p>
</div>
<p><br></p>
<p>While marginalisation serves for training, conditioning leads the key <span class="orange">GP prediction formula</span>:</p>
</div>
<div class="fragment">
<div class="block">
<p><span class="math display">\[f_{\color{purple}{m}} \mid f_{\color{grey}{o}} \sim \mathcal{N} \Big(
m_{\color{purple}{m}} + C_{\color{purple}{m}, \color{grey}{o}}  C_{\color{grey}{o, o}}^{-1} (f_{\color{grey}{o}} - m_{\color{grey}{o}}), \ \ C_{\color{purple}{m, m}} - C_{\color{purple}{m}, \color{grey}{o}}  C_{\color{grey}{o, o}}^{-1} C_{\color{grey}{o}, \color{purple}{m}} \Big)\]</span></p>
</div>
</div>
</section>
<section id="a-visual-explanation-of-gp-regression" class="slide level2">
<h2>A visual explanation of GP regression</h2>
<p><br></p>
<center>
<img data-src="images/illu_prior_gp.gif" style="width:49.0%"> <img data-src="images/illu_trained_gp.gif" style="width:49.0%">
</center>
</section>
<section id="updating-our-knowledge-about-functions" class="slide level2">
<h2>Updating our knowledge about functions</h2>
<br>
<center>
<img data-src="images/illu_post_gp1.gif" style="width:49.0%"> <img data-src="images/illu_post_gp1.png" style="width:49.0%">
</center>
</section>
<section id="updating-our-knowledge-about-functions-1" class="slide level2">
<h2>Updating our knowledge about functions</h2>
<br>
<center>
<img data-src="images/illu_post_gp2.gif" style="width:49.0%"> <img data-src="images/illu_post_gp2.png" style="width:49.0%">
</center>
</section>
<section id="updating-our-knowledge-about-functions-2" class="slide level2">
<h2>Updating our knowledge about functions</h2>
<br>
<center>
<img data-src="images/illu_post_gp3.gif" style="width:49.0%"> <img data-src="images/illu_post_gp3.png" style="width:49.0%">
</center>
</section>
<section id="forecasting-with-a-unique-gp" class="slide level2">
<h2>Forecasting with a unique GP</h2>
<center>
<img data-src="images/illu_gp.png" style="width:80.0%">
</center>
</section>
<section id="forecasting-with-a-unique-gp-1" class="slide level2">
<h2>Forecasting with a unique GP</h2>
<center>
<img data-src="images/illu_gp_gif.gif" style="width:80.0%">
</center>
</section>
<section id="time-to-fit-some-gps" class="slide level2">
<h2>Time to fit some GPs!</h2>
<p><img data-src="images/MagmaClustR_illu.gif" style="width:55.0%"> <img data-src="images/logo_magmaclustr.png" style="width:20.0%" alt="image alt >"></p>
<p>We will now extensively use the <a href="https://arthurleroy.github.io/MagmaClustR">MagmaClustR</a> package for modelling functional data with GPs.</p>
</section>
<section id="it-all-starts-with-observations-from-multiple-sources" class="slide level2">
<h2>It all starts with observations from multiple sources …</h2>
<p><br></p>
<center>
<img data-src="images/illu_multitask1.png" style="width:100.0%">
</center>
</section>
<section id="it-all-starts-with-observations-from-multiple-sources-1" class="slide level2">
<h2>It all starts with observations from multiple sources …</h2>
<p><br></p>
<center>
<img data-src="images/illu_multitask2.png" style="width:100.0%">
</center>
</section>
<section id="but-sometimes-we-are-not-measuring-the-same-things" class="slide level2">
<h2>… but sometimes we are not measuring the same things …</h2>
<center>
<img data-src="images/illu_gps_rugby_2_outputs.png" style="width:60.0%">
</center>
</section>
<section id="and-the-input-space-can-be-multidimensional-as-well" class="slide level2">
<h2>… and the input space can be multidimensional as well …</h2>
<center>
<img data-src="images/illu_2d_multi.png" style="width:90.0%">
</center>
</section>
<section id="among-many-other-difficulties" class="slide level2">
<h2>… among many other difficulties …</h2>
<center>
<img data-src="images/data1.png" style="width:100.0%">
</center>
<p>Several classical challenges:</p>
<ul>
<li><span class="orange">Irregular</span> measurements (in number of observations and location),</li>
</ul>
</section>
<section id="among-many-other-difficulties-1" class="slide level2">
<h2>… among many other difficulties …</h2>
<center>
<img data-src="images/data4.png" style="width:100.0%">
</center>
<p>Several classical challenges:</p>
<ul>
<li><span class="orange">Irregular</span> measurements (in number of observations and location),<br>
</li>
<li><span class="orange">Multiple</span> sources of data (athletes, sensors, …)</li>
</ul>
</section>
<section id="or-all-of-them-at-once" class="slide level2">
<h2>… or all of them at once!</h2>
<center>
<img data-src="images/illu_multimulti.png" style="width:90.0%">
</center>
</section>
<section id="multi-task-multi-output-what-are-the-maths-behind-that" class="slide level2">
<h2>Multi-Task? Multi-Output? What are the maths behind that?</h2>
<p>In the previous examples, we saw a variety of situations that naturally lead to the same mathematical formulation of the learning problem:</p>
<div class="block">
<p><span class="math display">\[y_s = \color{orange}{f_s}(x_s) + \epsilon_s, \hspace{3cm} \forall s = 1, \dots, S\]</span></p>
</div>
<p>Learn <span class="math inline">\(\color{orange}{f_s}\)</span>, the underlying relationship between <span class="math inline">\(x_s\)</span> and <span class="math inline">\(y_s\)</span>, for each source of data <span class="math inline">\(\color{orange}{s}\)</span>.</p>
<div class="fragment">
<p><br></p>
<p><span class="orange">Multi-</span>, in contrast with <span class="orange">single-</span>, regression implies that some <strong>information can be shared</strong> across data sources to improve learning/predictions.</p>
</div>
<div class="fragment">
<p><br></p>
<p>The nature of measurements leads to a more philosophical distinction:</p>
<ul>
<li>Multi-<span class="orange">Output</span> clearly refers to several variables of interest. An <em>Output</em> is a quantity we aim to infer/predict from <em>Input</em> measurements, and which may be correlated with others.</li>
<li>Multi-<span class="orange">Task</span> implies the existence of an underlying pattern, shared by several tasks or individuals, which can be jointly exploited to build a common model.</li>
</ul>
</div>
</section>
<section id="multi-task-gaussian-processes" class="slide level2">
<h2>Multi-Task Gaussian Processes</h2>
<div class="block">
<p><span class="math display">\[y_t = \mu_0 + f_t + \epsilon_t, \hspace{3cm} \forall t = 1, \dots, T\]</span></p>
</div>
<div class="columns">
<div class="column" style="width:60%;">
<p>with:</p>
<ul>
<li><span class="math inline">\(\mu_0 \sim \mathcal{GP}(m_0, K_{0}),\)</span></li>
<li><span class="math inline">\(f_t \sim \mathcal{GP}(0, \Sigma_{\theta_t}), \ \perp \!\!\! \perp_t,\)</span></li>
<li><span class="math inline">\(\epsilon_t \sim \mathcal{GP}(0, \sigma_t^2), \  \perp \!\!\! \perp_t.\)</span></li>
</ul>
</div><div class="column" style="width:40%;">
<p><img data-src="images/magma_graph.png" style="width:50.0%"></p>
</div></div>
<div class="fragment">
<p>It follows that:</p>
<div class="block">
<p><span class="math display">\[y_t \mid \mu_0 \sim \mathcal{GP}(\mu_0, \Sigma_{\theta_t} + \sigma_t^2 I), \ \perp \!\!\! \perp_t\]</span></p>
</div>
<p><span class="math inline">\(\rightarrow\)</span> Unified GP framework with a <span class="orange">shared mean</span> process <span class="math inline">\(\mu_0\)</span>, and <span class="orange">task-specific</span> process <span class="math inline">\(f_t\)</span>,<br>
<span class="math inline">\(\rightarrow\)</span> Naturaly <span class="orange">handles irregular grids</span> of input data.</p>
</div>
<div class="fragment">
<p><strong>Goal:</strong> Learn the hyper-parameters, (and <span class="math inline">\(\mu_0\)</span>’s hyper-posterior).<br>
<strong>Difficulty:</strong> The likelihood depends on <span class="math inline">\(\mu_0\)</span>, and tasks are <strong>not independent</strong>.</p>
</div>
</section>
<section id="notation-and-dimensionality" class="slide level2">
<h2>Notation and dimensionality</h2>
<p>Each task has its specific vector of inputs <span class="math inline">\(\color{purple}{\textbf{x}_i}\)</span> associated with outputs <span class="math inline">\(\textbf{y}_i\)</span>.<br>
The mean process <span class="math inline">\(\mu_0\)</span> requires to define <strong>pooled vectors</strong> and additional notation follows:</p>
<ul>
<li><span class="math inline">\(\textbf{y} = (\textbf{y}_1,\dots, \textbf{y}_i, \dots, \textbf{y}_M)^T,\)</span></li>
<li><span class="math inline">\(\color{grey}{\textbf{x}} = (\textbf{x}_1,\dots,\textbf{x}_i, \dots, \textbf{x}_M)^T,\)</span></li>
</ul>
<div class="fragment">
<ul>
<li><span class="math inline">\(\textbf{K}_{0}^{\color{grey}{\textbf{x}}}\)</span>: covariance matrix from the process <span class="math inline">\(\mu_0\)</span> evaluated on <span class="math inline">\(\color{grey}{\textbf{x}},\)</span></li>
<li><span class="math inline">\(\boldsymbol{\Sigma}_{\theta_t}^{\color{purple}{\textbf{x}_i}}\)</span>: covariance matrix from the process <span class="math inline">\(f_t\)</span> evaluated on <span class="math inline">\(\color{purple}{\color{purple}{\textbf{x}_i}},\)</span></li>
</ul>
</div>
<div class="fragment">
<ul>
<li><span class="math inline">\(\Theta = \{(\theta_t)_i, \sigma_t^2 \}\)</span>: the set of hyper-parameters,</li>
<li><span class="math inline">\(\boldsymbol{\Psi}_{\theta_t, \sigma_t^2}^{\color{purple}{\textbf{x}_i}} = \boldsymbol{\Sigma}_{\theta_t}^{\color{purple}{\textbf{x}_i}} + \sigma_t^2 I_{N_i}\)</span>.</li>
</ul>
<p><br></p>
<p>While GP are infinite-dimensional objects, a tractable inference on a finite set of observations <span class="orange">fully determines</span> the overall properties. <br></p>
</div>
</section>
<section id="em-algorithm" class="slide level2">
<h2>EM algorithm</h2>
<p><strong>E-step</strong></p>
<div class="block">
<p><span class="math display">\[
\begin{align}
  p(\mu_0(\color{grey}{\mathbf{x}}) \mid \textbf{y}, \hat{\Theta})
  &amp;\propto \mathcal{N}(\mu_0(\color{grey}{\mathbf{x}}); m_0(\color{grey}{\textbf{x}}), \textbf{K}_{0}^{\color{grey}{\textbf{x}}}) \times \prod\limits_{t =1}^T \mathcal{N}(\mathbf{y}_t;  \mu_0( \color{purple}{\textbf{x}_i}), \boldsymbol{\Psi}_{\hat{\theta}_i, \hat{\sigma}_i^2}^{\color{purple}{\textbf{x}_i}}) \\
  &amp;= \mathcal{N}(\mu_0(\color{grey}{\mathbf{x}});  \hat{m}_0(\color{grey}{\textbf{x}}), \hat{\textbf{K}}^{\color{grey}{\textbf{x}}}),
\end{align}
\]</span></p>
</div>
<ul>
<li><span class="math inline">\(\hat{\textbf{K}}^{\color{grey}{\textbf{x}}} = ({\textbf{K}_{0}^{\color{grey}{\textbf{x}}}}^{-1} + \sum\limits_{t = 1}^T {\boldsymbol{\Psi}_{\hat{\theta}_i, \hat{\sigma}_i^2}^{\color{purple}{\textbf{x}_i}}}^{-1})^{-1}\)</span></li>
<li><span class="math inline">\(\hat{m}_0(\color{grey}{\textbf{x}}) = \hat{\textbf{K}}^{\color{grey}{\textbf{x}}}({\textbf{K}_{0}^{\color{grey}{\textbf{x}}}}^{-1} m_0(\color{grey}{\mathbf{x}}) + \sum\limits_{t = 1}^T {\boldsymbol{\Psi}_{\hat{\theta}_i, \hat{\sigma}_i^2}^{\color{purple}{\textbf{x}_i}}}^{-1} \mathbf{y}_t)\)</span>.</li>
</ul>
<div class="fragment">
<p><strong>M-step</strong></p>
<div class="block">
<p><span class="math display">\[
\begin{align*}
    \hat{\Theta}
    &amp;= \underset{\Theta}{\arg\max} \ \   \sum\limits_{t = 1}^{T}\left\{  \log  \mathcal{N} \left( \mathbf{y}_t; \hat{m}_0(\color{purple}{\mathbf{x}_t}), \boldsymbol{\Psi}_{\theta_t, \sigma^2}^{\color{purple}{\mathbf{x}_t}}  \right) - \dfrac{1}{2} Tr \left(   \hat{\mathbf{K}}^{\color{purple}{\mathbf{x}_t}} {\boldsymbol{\Psi}_{\theta_t, \sigma^2}^{\color{purple}{\mathbf{x}_t}}}^{-1}  \right)  \right\}.
\end{align*}
\]</span></p>
</div>
</div>
</section>
<section id="covariance-structure-assumption-and-computational-complexity" class="slide level2">
<h2>Covariance structure assumption and computational complexity</h2>
<p>Sharing the covariance structures or not offers a compromise between <span class="orange">flexibility</span> and <span class="orange">parsimony</span>:</p>
<ul>
<li><strong>One</strong> optimisation problem for <span class="math inline">\(\theta\)</span>, and a <span class="orange">shared</span> process for all tasks,<br>
</li>
<li><span class="math inline">\(\color{blue}{T}\)</span> distinct optimisation problems for <span class="math inline">\(\{\theta_t\}_t\)</span>, and <span class="orange">task-specific</span> processes.</li>
</ul>
<p><br></p>
<div class="fragment">
<p>Major interests:</p>
<ul>
<li>Both approaches <span class="orange">scale linearly</span> with the number of tasks,<br>
</li>
<li>Parallel computing can be used to speed up training.</li>
</ul>
<p><br></p>
</div>
<div class="fragment">
<p>Overall, the <span class="orange">computational complexity</span> is: <span class="math display">\[
\mathcal{O}(\color{blue}{T} \times N_t^3 + N^3)
\]</span></p>
<p>with <span class="math inline">\(N = \bigcup\limits_{t = 1}^\color{blue}{T} N_t\)</span></p>
</div>
</section>
<section id="predictions-with-multi-task-gps" class="slide level2">
<h2>Predictions with Multi-Task GPs</h2>
<ul>
<li>Multi-Task prior:</li>
</ul>
<div class="block">
<p><span class="math display">\[p \left( \begin{bmatrix}
                 y_*(\color{grey}{\mathbf{x}_{*}}) \\
                 y_*(\color{purple}{\mathbf{x}^{p}}) \\
               \end{bmatrix} \mid \textbf{y} \right) = \mathcal{N} \left(
               \begin{bmatrix}
                 y_*(\color{grey}{\mathbf{x}_{*}})  \\
                 y_*(\color{purple}{\mathbf{x}^{p}}) \\
               \end{bmatrix}; \
              \begin{bmatrix}
                 \hat{m}_0(\color{grey}{\mathbf{x}_{*}})  \\
                 \hat{m}_0(\color{purple}{\mathbf{x}^{p}}) \\
               \end{bmatrix},
              \begin{pmatrix}
                 \Gamma_{\color{grey}{**}} &amp; \Gamma_{\color{grey}{*}\color{purple}{p}} \\
                 \Gamma_{\color{purple}{p}\color{grey}{*}} &amp; \Gamma_{\color{purple}{pp}}
              \end{pmatrix} \right)\]</span></p>
</div>
<div class="fragment">
<ul>
<li>Multi-Task posterior:</li>
</ul>
<div class="block">
<p><span class="math display">\[p(y_*(\color{purple}{\mathbf{x}^{p}})  \mid y_*(\color{grey}{\mathbf{x}_{*}}), \textbf{y}) = \mathcal{N} \Big( y_*(\color{purple}{\mathbf{x}^{p}}); \ \hat{\mu}_{*}(\color{purple}{\mathbf{x}^{p}}) , \hat{\Gamma}_{\color{purple}{pp}} \Big)\]</span></p>
</div>
</div>
<div class="fragment">
<p>with:</p>
<div class="block">
<ul>
<li><span class="math inline">\(\hat{\mu}_{*}(\color{purple}{\mathbf{x}^{p}}) =  \hat{m}_0(\color{purple}{\mathbf{x}^{p}}) + \Gamma_{\color{purple}{p}\color{grey}{*}}\Gamma_{\color{grey}{**}}^{-1} (y_*(\color{grey}{\mathbf{x}_{*}}) - \hat{m}_0 (\color{grey}{\mathbf{x}_{*}}))\)</span></li>
<li><span class="math inline">\(\hat{\Gamma}_{\color{purple}{pp}} = \Gamma_{\color{purple}{pp}} - \Gamma_{\color{purple}{p}\color{grey}{*}}\Gamma_{\color{grey}{**}}^{-1} \Gamma_{\color{grey}{*}\color{purple}{p}}\)</span></li>
</ul>
</div>
</div>
</section>
<section id="a-gif-is-worth-a-thousand-words" class="slide level2">
<h2>A GIF is worth a thousand words</h2>
<center>
<img data-src="images/illu_magma.png" style="width:80.0%">
</center>
</section>
<section id="a-gif-is-worth-a-thousand-words-1" class="slide level2">
<h2>A GIF is worth a thousand words</h2>
<center>
<img data-src="images/illu_magma_gif.gif" style="width:80.0%">
</center>
</section>
<section id="comparison-with-single-gp-regression" class="slide level2">
<h2>Comparison with single GP regression</h2>
<center>
<img data-src="images/illu_GP_forecast.png" style="width:49.0%"> <img data-src="images/illu_magma_forecast.png" style="width:49.0%">
</center>
</section>
<section id="yet-another-comparison-with-single-gp-regression-but-in-2d" class="slide level2">
<h2>Yet another comparison with single GP regression but in 2D</h2>
<center>
<img data-src="images/multidim_GP.png" style="width:49.0%"><img data-src="images/multidim_magma.png" style="width:49.0%">
</center>
<p>Multi-Task GPs regression provides more reliable predictions when individual processes are sparsely observed, while greatly reducing the associated uncertainty</p>
</section>
<section id="your-turn-to-work-again" class="slide level2">
<h2>Your turn to work again</h2>
<center>
<img data-src="images/gif_heatmap.gif" style="width:80.0%">
</center>
</section>
<section id="adding-some-clustering-into-multi-task-gps" class="slide level2">
<h2>Adding some clustering into Multi-Task GPs</h2>
<p>A <strong>unique</strong> underlying mean process might be too restrictive.</p>
<p><span class="math inline">\(\rightarrow\)</span> <span class="orange">Mixture</span> of multi-task GPs:</p>
<div class="block">
<p><span class="math display">\[y_t = \mu_0 + f_t + \epsilon_t, \hspace{3cm} \forall t = 1, \dots, T\]</span></p>
</div>
<p>with:</p>
<div>
<ul>
<li class="fragment"><span class="math inline">\(\color{green}{Z_{t}} \sim \mathcal{M}(1, \color{green}{\boldsymbol{\pi}}), \ \perp \!\!\! \perp_t,\)</span></li>
</ul>
</div>
<ul>
<li><span class="math inline">\(\mu_0 \sim \mathcal{GP}(m_0, K_0),\)</span></li>
<li><span class="math inline">\(f_t \sim \mathcal{GP}(0, \Sigma_{\theta_t}), \ \perp \!\!\! \perp_t,\)</span></li>
<li><span class="math inline">\(\epsilon_t \sim \mathcal{GP}(0, \sigma_t^2), \  \perp \!\!\! \perp_t.\)</span></li>
</ul>
<p>It follows that:</p>
<div class="block">
<p><span class="math display">\[y_t \mid \mu_0 \sim \mathcal{GP}(\mu_0, \Psi_t), \ \perp \!\!\! \perp_t\]</span></p>
</div>
</section>
<section id="adding-some-clustering-into-multi-task-gps-1" class="slide level2">
<h2>Adding some clustering into Multi-Task GPs</h2>
<p>A <strong>unique</strong> underlying mean process might be too restrictive.</p>
<p><span class="math inline">\(\rightarrow\)</span> <span class="orange">Mixture</span> of multi-task GPs:</p>
<div class="block">
<p><span class="math display">\[y_t \mid \{\color{green}{Z_{tk}} = 1 \} = \mu_{\color{green}{k}} + f_t + \epsilon_t, \hspace{3cm} \forall t = 1, \dots, T\]</span></p>
</div>
<div class="columns">
<div class="column" style="width:60%;">
<p>with:</p>
<ul>
<li><span class="math inline">\(\color{green}{Z_{t}} \sim \mathcal{M}(1, \color{green}{\boldsymbol{\pi}}), \ \perp \!\!\! \perp_t,\)</span></li>
<li><span class="math inline">\(\mu_{\color{green}{k}} \sim \mathcal{GP}(m_{\color{green}{k}}, \color{green}{C_{{k}}})\ \perp \!\!\! \perp_{\color{green}{k}},\)</span></li>
<li><span class="math inline">\(f_t \sim \mathcal{GP}(0, \Sigma_{\theta_t}), \ \perp \!\!\! \perp_t,\)</span></li>
<li><span class="math inline">\(\epsilon_t \sim \mathcal{GP}(0, \sigma_t^2), \  \perp \!\!\! \perp_t.\)</span></li>
</ul>
</div><div class="column" style="width:40%;">
<p><img data-src="images/magmaclust_graph.png" style="width:60.0%"></p>
</div></div>
<p>It follows that:</p>
<div class="block">
<p><span class="math display">\[y_t \mid \mu_0 \sim \mathcal{GP}(\mu_0, \Psi_t), \ \perp \!\!\! \perp_t\]</span></p>
</div>
</section>
<section id="adding-some-clustering-into-multi-task-gps-2" class="slide level2">
<h2>Adding some clustering into Multi-Task GPs</h2>
<p>A <strong>unique</strong> underlying mean process might be too restrictive.</p>
<p><span class="math inline">\(\rightarrow\)</span> <span class="orange">Mixture</span> of multi-task GPs:</p>
<div class="block">
<p><span class="math display">\[y_t \mid \{\color{green}{Z_{ik}} = 1 \} = \mu_{\color{green}{k}} + f_t + \epsilon_t, \hspace{3cm} \forall t = 1, \dots, T\]</span></p>
</div>
<div class="columns">
<div class="column" style="width:60%;">
<p>with:</p>
<ul>
<li><span class="math inline">\(\color{green}{Z_{t}} \sim \mathcal{M}(1, \color{green}{\boldsymbol{\pi}}), \ \perp \!\!\! \perp_t,\)</span></li>
<li><span class="math inline">\(\mu_{\color{green}{k}} \sim \mathcal{GP}(m_{\color{green}{k}}, \color{green}{C_{{k}}})\ \perp \!\!\! \perp_{\color{green}{k}},\)</span></li>
<li><span class="math inline">\(f_t \sim \mathcal{GP}(0, \Sigma_{\theta_t}), \ \perp \!\!\! \perp_t,\)</span></li>
<li><span class="math inline">\(\epsilon_t \sim \mathcal{GP}(0, \sigma_t^2), \  \perp \!\!\! \perp_t.\)</span></li>
</ul>
</div><div class="column" style="width:40%;">
<p><img data-src="images/magmaclust_graph.png" style="width:60.0%"></p>
</div></div>
<p>It follows that:</p>
<div class="block">
<p><span class="math display">\[y_t \mid \{  \boldsymbol{\mu} , \color{green}{\boldsymbol{\pi}} \} \sim \sum\limits_{k=1}^K{ \color{green}{\pi_k} \ \mathcal{GP}\Big(\mu_{\color{green}{k}},   \Psi_t^\color{green}{k} \Big)}, \ \perp \!\!\! \perp_t\]</span></p>
</div>
</section>
<section id="learning" class="slide level2">
<h2>Learning</h2>
<p>The integrated likelihood is <strong>not tractable</strong> anymore due to <span class="orange">posterior dependencies</span> between <span class="math inline">\( \boldsymbol{\mu} = \{\mu_\color{green}{k}\}_\color{green}{k}\)</span> and <span class="math inline">\(\mathbf{Z}= \{Z_t\}_t\)</span>.</p>
<div class="fragment">
<p><br></p>
<p>Variational inference still allows us to maintain <strong>closed-form</strong> approximations. For any distribution <span class="math inline">\(q\)</span>:</p>
<div class="block">
<p><span class="math display">\[\log p(\textbf{y} \mid \Theta) = \mathcal{L}(q; \Theta) + KL \big( q \mid \mid p(\boldsymbol{\mu}, \boldsymbol{Z} \mid \textbf{y}, \Theta)\big)\]</span></p>
</div>
<p><br></p>
</div>
<div class="fragment">
<p>The posterior independence is <em>induced</em> by a <span class="orange">mean-field assumption</span>:</p>
<div class="block">
<p><span class="math display">\[q(\boldsymbol{\mu}, \boldsymbol{Z}) = q_{\boldsymbol{\mu}}(\boldsymbol{\mu})q_{\boldsymbol{Z}}(\boldsymbol{Z}).\]</span></p>
</div>
<p><br></p>
<div>
<p>Maximising the <strong>lower bound</strong> <span class="math inline">\(\mathcal{L}(q; \Theta)\)</span> induces natural <span class="orange">factorisations</span> over clusters and tasks for the variational distributions.</p>
</div>
</div>
</section>
<section id="variational-em" class="slide level2">
<h2>Variational EM</h2>
<div class="theorem">
<p>E step: <span class="math display">\[
\begin{align}
\hat{q}_{\boldsymbol{\mu}}(\boldsymbol{\mu}) &amp;= \color{green}{\prod\limits_{k = 1}^K} \mathcal{N}(\mu_\color{green}{k};\hat{m}_\color{green}{k}, \hat{\textbf{C}}_\color{green}{k}) , \hspace{2cm}
\hat{q}_{\boldsymbol{Z}}(\boldsymbol{Z}) = \prod\limits_{t = 1}^T \mathcal{M}(Z_t;1, \color{green}{\boldsymbol{\tau}_t})
\end{align}
\]</span> M step:</p>
<p><span class="math display">\[
\begin{align*}
    \hat{\Theta}
    &amp;= \underset{\Theta}{\arg\max} \sum\limits_{k = 1}^{K}\sum\limits_{t = 1}^{T}\tau_{tk}\ \mathcal{N} \left( \mathbf{y}_t; \ \hat{m}_k, \boldsymbol{\Psi}_{\color{blue}{\theta_t}, \color{blue}{\sigma_t^2}}  \right) - \dfrac{1}{2} \textrm{tr}\left( \mathbf{\hat{C}}_k\boldsymbol{\Psi}_{\color{blue}{\theta_t}, \color{blue}{\sigma_t^2}}^{-1}\right) \\
        &amp; \hspace{1cm} + \sum\limits_{k = 1}^{K}\sum\limits_{t = 1}^{T}\tau_{tk}\log \color{green}{\pi_{k}}
\end{align*}
\]</span></p>
</div>
</section>
<section id="cluster-specific-and-mixture-predictions" class="slide level2">
<h2>Cluster-specific and mixture predictions</h2>
<p><br></p>
<ul>
<li>Multi-Task posterior for each cluster:</li>
</ul>
<div class="block">
<p><span class="math display">\[
p(y_*(\mathbf{x}^{p})  \mid \color{green}{Z_{*k}} = 1, y_*(\mathbf{x}_{*}), \textbf{y}) = \mathcal{N} \Big( y_*(\mathbf{x}^{p}); \ \hat{\mu}_{*}^\color{green}{k}(\mathbf{x}^{p}) , \hat{\Gamma}_{pp}^\color{green}{k} \Big), \forall \color{green}{k},
\]</span></p>
</div>
<div class="block">
<p><span class="math inline">\(\hat{\mu}_{*}^\color{green}{k}(\mathbf{x}^{p}) =  \hat{m}_\color{green}{k}(\mathbf{x}^{p}) + \Gamma^\color{green}{k}_{p*} {\Gamma^\color{green}{k}_{**}}^{-1} (y_*(\mathbf{x}_{*}) - \hat{m}_\color{green}{k} (\mathbf{x}_{*}))\)</span><br>
<span class="math inline">\(\hat{\Gamma}_{pp}^\color{green}{k} = \Gamma_{pp}^\color{green}{k} - \Gamma_{p*}^\color{green}{k} {\Gamma^{\color{green}{k}}_{**}}^{-1} \Gamma^{\color{green}{k}}_{*p}\)</span></p>
</div>
<p><br></p>
<div class="fragment">
<ul>
<li>Predictive multi-task GPs mixture:</li>
</ul>
<div class="block">
<p><span class="math display">\[p(y_*(\textbf{x}^p) \mid y_*(\textbf{x}_*), \textbf{y}) = \color{green}{\sum\limits_{k = 1}^{K} \tau_{*k}} \ \mathcal{N} \big( y_*(\mathbf{x}^{p}); \ \hat{\mu}_{*}^\color{green}{k}(\textbf{x}^p) , \hat{\Gamma}_{pp}^\color{green}{k}(\textbf{x}^p) \big).\]</span></p>
</div>
</div>
</section>
<section id="an-image-is-still-worth-many-words" class="slide level2">
<h2>An image is still worth many words</h2>
<center>
<img data-src="images/illu_magmaclust1.png" style="width:85.0%">
</center>
<p>A unique mean process can struggle to capture relevant signals in presence of group structures.</p>
</section>
<section id="an-image-is-still-worth-many-words-1" class="slide level2">
<h2>An image is still worth many words</h2>
<center>
<img data-src="images/illu_magmaclust2.png" style="width:85.0%">
</center>
<p>By identifying the underlying clustering structure, we can <span class="orange">discard unnecessary information</span> and provide enhanced predictions as well as lower uncertainty.</p>
</section>
<section id="saved-by-the-weights" class="slide level2">
<h2>Saved by the weights</h2>
<br>
<center>
<img data-src="images/illu_2_other_clusters.png" style="width:100.0%">
</center>
<p>Each cluster-specific prediction is weighted by its membership probability <span class="math inline">\(\color{green}{\tau_{*k}}\)</span>.</p>
</section>
<section id="practical-application-follow-up-of-bmi-during-childhood" class="slide level2">
<h2>Practical application: follow up of BMI during childhood</h2>
<br>
<center>
<img data-src="images/illu_clusters.png" style="width:100.0%">
</center>
</section>
<section id="reconstruction-for-sparse-or-incomplete-data" class="slide level2">
<h2>Reconstruction for sparse or incomplete data</h2>
<center>
<img data-src="images/missing_5clusters_0to10years.png" style="width:80.0%">
</center>
</section>
<section id="forecasting-long-term-evolution-of-bmi" class="slide level2">
<h2>Forecasting long term evolution of BMI</h2>
<center>
<img data-src="images/pred_5clusters_0to10years.png" style="width:80.0%">
</center>
</section>
<section id="leveraging-the-uncertainty-quantification-of-gps" class="slide level2">
<h2>Leveraging the uncertainty quantification of GPs</h2>
<center>
<img data-src="images/male_risk_obesity.png" style="width:55.0%">
</center>
</section>
<section id="deriving-downstream-tools-for-practitioners" class="slide level2">
<h2>Deriving downstream tools for practitioners</h2>
<p>Uncertainty quantification provides valuable information to practitioners to make decisions based on probabilistic statements, like the <span class="orange">risk of overweight at 10 years</span>.</p>
<center>
<img data-src="images/overweight_risk.png" style="width:60.0%">
</center>
</section>
<section id="your-time-to-shine" class="slide level2">
<h2>Your time to shine!</h2>
<p><br></p>
<p>How about we try to cluster swimmers and forecast their performances?</p>
<center>
<img data-src="images/pred_example_women_swimming_data.png" style="width:49.0%"><img data-src="images/pred_example_men_swimming_data.png" style="width:49.0%">
</center>
</section>
<section id="multi-output-gps-another-paradigm-based-on-covariance-structures" class="slide level2">
<h2>Multi-Output GPs: another paradigm based on covariance structures</h2>
<center>
<img data-src="images/Tide_prediction_ind1.png" style="width:28.0%"><img data-src="images/Tide_prediction_ind4.png" style="width:28.0%">
</center>
<center>
<img data-src="images/Tide_prediction_sppitc1.png" style="width:28.0%"><img data-src="images/Tide_prediction_sppitc4.png" style="width:28.0%">
</center>
</section>
<section id="multi-task-multi-output-gps-best-of-both-worlds" class="slide level2">
<h2>Multi-Task-Multi-Output GPs: best of both worlds?</h2>
<ul>
<li><span class="orange">Multi-Output GPs</span> derive full rank covariance matrices to exploit cross-correlations between all Input-Output pairs. They are <strong>flexible</strong>, <strong>expressive</strong>, and well-designed to recover very <span class="orange">different signals</span> or measurements. On the down side, they are computationally expensive, sometimes difficult to train in practice, and poorly adapted to modelling multiple tasks/individuals.</li>
</ul>
<div class="fragment">
<ul>
<li><span class="orange">Multi-Task GPs</span> leverage latent mean processes to share information on the common trends and properties of the data. They are <strong>parsimonious</strong>, <strong>efficient</strong>, and well-designed to capture <span class="orange">common patterns</span> for multiple tasks/individuals on the same variable of interest. On the down side, they can be limited to fully capture correlations between the GPs and exploit initial knowledge that collected data may be of different natures.</li>
</ul>
</div>
<div class="fragment">
<p><br></p>
<p>Both work on separate parts of a GP, with no assumption on the other, and are theoretically compatible. They are even expected to <span class="orange">synergise well</span>, by tackling each others limitations.</p>
</div>
</section>
<section id="generative-model-of-momt-gps" class="slide level2">
<h2>Generative model of MOMT GPs</h2>
<div class="block">
<p><span class="math display">\[\begin{bmatrix} y_t^{1} \\ \vdots \\ y_t^{O} \\ \end{bmatrix} =
\begin{bmatrix} \mu_0 \\ \vdots \\ \mu_0 \\ \end{bmatrix} +
\begin{bmatrix} f_t^{1} \\ \vdots \\ f_t^{O} \\ \end{bmatrix} +
\begin{bmatrix} \epsilon_t^{1} \\ \vdots \\ \epsilon_t^{O} \\ \end{bmatrix}, \hspace{3cm} \forall t = 1, \dots, T\]</span></p>
</div>
<ul>
<li><span class="math inline">\(\mu_0 \sim \mathcal{GP}(m_0, K_0),\)</span></li>
<li><span class="math inline">\(\begin{bmatrix} f_t^{1} \\ \vdots \\ f_t^{O} \\ \end{bmatrix} \sim \mathcal{GP}(0, \Sigma_{\theta_t}), \ \perp \!\!\! \perp_t,\)</span></li>
<li><span class="math inline">\(\begin{bmatrix} \epsilon_t^{1} \\ \vdots \\ \epsilon_t^{O} \\ \end{bmatrix} \sim \mathcal{GP}(0, \begin{bmatrix} {\sigma_t^{1}}^O \\ \vdots \\ {\sigma_{t}^O}^2 \\ \end{bmatrix} \times I_O), \  \perp \!\!\! \perp_t.\)</span></li>
</ul>
<p>All mathematical objects from Multi-Task GPs are now extended with stacked Outputs.</p>
</section>
<section id="process-convolution-covariance-structure-and-inference" class="slide level2">
<h2>Process convolution covariance structure and inference</h2>
<div class="block">
<p><span class="math display">\[\Big[k_{\theta_t}(\mathbf{x}, \mathbf{x}^{\prime})\Big]_{o,o'} = \dfrac{S_{t,o} \ S_{t,o{\prime}}}{(2\pi)^{D/2} \ |\Sigma|^{1/2}} \ \exp\Big(-\dfrac{1}{2}(\mathbf{x} - \mathbf{x}^{\prime})^T \Sigma^{-1}(\mathbf{x}-\mathbf{x}^{\prime})\Big)\]</span></p>
</div>
<p>with:</p>
<div class="block">
<ul>
<li><span class="math inline">\(S_{t,o}, S_{t,o{\prime}} \in \mathbb{R}\)</span>, the Output-specific variance terms,</li>
<li><span class="math inline">\(\Sigma = P_{t,o}^{-1}+P_{t,o^{\prime}}^{-1}+\Lambda^{-1} \in \mathbb{R}^{D \times D}\)</span>.</li>
</ul>
</div>
<div class="fragment">
<p>These matrices are typically diagonal, filled with Output-specific and latent lengthscales, one per Input dimension. In the end, we need to optimise <span class="math inline">\(\color{red}{O}\times \color{blue}{T} \times (3D+1)\)</span> hyper-parameters.</p>
<div class="block">
<h4 id="e-step">E-step</h4>
<p><span class="math display">\[
\begin{align}
  p(\mu_0(\color{grey}{\mathbf{x}}) \mid \textbf{y}, \hat{\Theta})
  = \mathcal{N}(\mu_0(\color{grey}{\mathbf{x}});  \hat{m}_0(\color{grey}{\textbf{x}}), \hat{\textbf{K}}^{\color{grey}{\textbf{x}}}),
\end{align}
\]</span></p>
<h4 id="m-step">M-step</h4>
<p><span class="math display">\[
\begin{align*}
    \hat{\Theta}
    &amp;= \underset{\Theta}{\arg\max} \sum\limits_{t = 1}^{T}\left\{  \log  \mathcal{N} \left( \mathbf{y}_t; \hat{m}_0(\color{purple}{\mathbf{x}_t^O}), \boldsymbol{\Psi}_{\theta_t, \sigma_t^2}^{\color{purple}{\mathbf{x}_t^O}}  \right) - \dfrac{1}{2} Tr \left(   \hat{\mathbf{K}}^{\color{purple}{\mathbf{x}_t^O}} {\boldsymbol{\Psi}_{\theta_t, \sigma_t^2}^{\color{purple}{\mathbf{x}_t^O}}}^{-1}  \right)  \right\}.
\end{align*}
\]</span></p>
</div>
</div>
</section>
<section id="are-we-still-good-at-forecasting-from-the-mean" class="slide level2">
<h2>Are we still good at forecasting from the mean?</h2>
<center>
<img data-src="images/pred_MOMT_for_MT.png" style="width:90.0%">
</center>
</section>
<section id="are-we-still-good-at-reconstructing-from-the-other-outputs" class="slide level2">
<h2>Are we still good at reconstructing from the other Outputs?</h2>
<center>
<img data-src="images/pred_MOMT_for_MO.png" style="width:90.0%">
</center>
</section>
<section id="do-we-really-get-the-best-of-both-worlds" class="slide level2">
<h2>Do we really get the best of both worlds?</h2>
<center>
<img data-src="images/pred_MOMT_full.png" style="width:90.0%">
</center>
</section>
<section id="take-home-messages" class="slide level2">
<h2>Take home messages</h2>
<div>
<ul>
<li class="fragment">Multi-Output / Multi-task: the same mathematical problem can hide <span class="orange">different intuitions</span> on the data and lead to dedicated modelling paradigms,</li>
<li class="fragment">Both the mean and covariance parameters can effectively be leveraged to <span class="orange">share information</span>,</li>
<li class="fragment"><span class="orange">Modelling jointly</span> data coming from several sources can costs roughly the same as independant modelling, <strong>but</strong> can improve dramatically predictive capacities.</li>
</ul>
</div>
<div class="fragment">
<p><strong>Rule of thumb:</strong></p>
<div style="white-space:nowrap;">
<table class="caption-top">
<colgroup>
<col style="width: 30%">
<col style="width: 19%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Output nature</th>
<th>Framework</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Independant variables of interest</td>
<td>Single-Task</td>
<td><span class="math inline">\(\color{red}{O}\times\mathcal{O}(N^3)\)</span></td>
</tr>
<tr class="even">
<td>Correlated variables of interest</td>
<td>Multi-Output</td>
<td><span class="math inline">\(\mathcal{O}(\color{red}{O}^3 N^3)\)</span></td>
</tr>
<tr class="odd">
<td>Occurences of the same phenomenon</td>
<td>Multi-Task</td>
<td><span class="math inline">\(\mathcal{O}(\color{blue}{T} \times N^3)\)</span></td>
</tr>
<tr class="even">
<td>Occurences of correlated variables</td>
<td>Multi-Task-Multi-Output</td>
<td><span class="math inline">\(\mathcal{O}(\color{blue}{T}\times\color{red}{O}^3  N^3)\)</span></td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="hope-you-now-mathbbptextlove-functional-data-textlearning-with-us-approx-1" class="slide level2">
<h2>Hope you now <span class="math inline">\(\mathbb{P}(\text{love functional data} | \text{learning with us}) \approx 1\)</span></h2>
<center>
Thank you for your attention<br>
<img data-src="images/logo_stat_sport.png" style="width:30.0%"><br>
Do not hesitate to reach out and join <a href="https://www.sfds.asso.fr/"><span class="orange">SFdS</span></a> and the <a href="https://www.sfds.asso.fr/fr/statistique_et_sport/618-statistique_et_sport/"><span class="orange">Statistique et Sport</span></a> group!
</center>


</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="slides_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="slides_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="slides_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="slides_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="slides_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="slides_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="slides_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="slides_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="slides_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="slides_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1280,

        height: 720,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>